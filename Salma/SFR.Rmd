---
title: "Cleaning and Regressions"
output: pdf_document
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Import Packages + Read in Data

```{r, include = FALSE}
library(tidyverse)
library(magrittr)
library(here)
library(data.table)
library(MASS)
library(stargazer)
library(broom)
library(sjPlot)
library(lubridate)

# read in csv
thesis_data <- read_csv(here("DATA","thesis-data.csv"))
```

### Cheatsheet
thesis_data <- raw data from csv into dataframe format

SFR <- all data where proprty type is single family residential

test1 <- creating flags for cleaning vars

dat1 <- applying flags to trim dataset

### Create SFR Dataframe

```{r, echo = FALSE}

# Create Data Frames
SFR <- thesis_data %>%
  filter(prop_type == "Single-family")

# Check for duplicates one more time
# All equal zero, meaning there are no duplicates of each of these variables
# Means we can use any one of them as a way of IDing observations uniquely
sum(data.frame(table(SFR$STATE_ID))$Freq > 1) 
sum(data.frame(table(SFR$RNO))$Freq > 1) 
sum(data.frame(table(SFR$PROPERTYID))$Freq > 1) 
sum(data.frame(table(SFR$TLID))$Freq > 1) 


# QUESTIONS
# What to do with non-conforming? Look up on PDX Maps
```

SFR data frame has 27,200 observations initially.

No observation repeat in any of the data frames acording to the unique IDs we have recorded for them. 

### Clean SFR Dataframe

```{r, echo = FALSE}

#-------------------- BEGINNING RYAN CODE ------------------------

test1 <- SFR %>%
  dplyr::select(-c(OWNER2, OWNER3, OWNERZIP, 
            MKTVALYR1, MKTVALYR2,
            BLDGVAL1, BLDGVAL2,
            LANDVAL1, LANDVAL2,  
            TOTALVAL1, TOTALVAL2,
            MS_GRADE, ES_GRADE,
            LEGAL_DESC, TAXCODE, PROP_CODE, LANDUSE, BLDGSQFT, BEDROOMS, ACC_STATUS, NAME, COMMPLAN, SHARED, COALIT, HORZ_VERT, AUDIT_NBRH, MIDDLE_SCH,  Category, SOURCE, FRONTAGE, COUNTY, YEARBUILT, bldgtype)) %>%
  mutate(top_1 =  SALEPRICE > quantile(SALEPRICE, .99),
         price_diff = SALEPRICE - LANDVAL3, 
         price_ratio = SALEPRICE/LANDVAL3 * 100,
         vacant_dummy = PRPCD_DESC == "VACANT LAND",
         llc_flag = grepl("LLC", OWNER1),
         proud_flag =  grepl("PROUD", OWNER1),
         trust_flag = grepl("TRUST", OWNER1)) %>%
  mutate(arms_length = price_ratio > 20,
         na_if(yearbuilt, 0))

# switch the NAs in the constraints to 0s
to0 <- function(x){ifelse(is.na(x), 0, x)}

#------------------------- END RYAN CODE ----------------------------

# removing observations 
dat1 <- test1 %>% #27,200
  filter(top_1 == FALSE, #26,931
         vacant_dummy == FALSE, #26,095
         arms_length == TRUE, #25,535
         proud_flag == FALSE, #25,520
         #trust_flag == FALSE -- these might be ones to take out on a case by case basis
  )

#-------------------- BEGINNING RYAN CODE ------------------------

#garage sqft
gar_sqft_sum <- dat1 %>%
  dplyr::select(matches("gar")) %>%
  rowSums()

# attic sqft
attic_sqft_sum <- dat1 %>%
  dplyr::select(matches("attic")) %>%
  rowSums()

# basement sqft
bsmt_sqft_sum <- dat1 %>%
  dplyr::select(matches("bsmt")) %>%
  dplyr::select(-c("BSMT PARKING","BSMT GAR")) %>%
  rowSums()
  
dat1 %<>%
  mutate(garage_sqft = gar_sqft_sum,
         garage_dum = garage_sqft > 0,
         attic_sqft = attic_sqft_sum,
         attic_dum = attic_sqft > 0,
         bsmt_sqft = bsmt_sqft_sum,
         bsmt_dum = bsmt_sqft > 0,
         saledate = mdy(saledate), 
         year_sold = year(saledate))

#------------------------- END RYAN CODE ----------------------------


  #%>%
  #recode(MKTVALYR, "09/27/17" = "2017")


# test2 <- SFR %>%
#     as.Date(MKTVALYR, saledate)
# check if saledate is the same year as market year


 # dat1 %<>% filter(yearbuilt == 1008.000)
# deal with this


ggplot(test1, aes(x = SALEPRICE, y = LANDVAL3)) +
  geom_point() 

summary(test1$top_1)
summary(test1$price_diff)
summary(test1$price_ratio)
summary(test1$vacant_dummy)
summary(test1$arms_length)
summary(test1$proud_flag)
summary(test1$trust_flag)


## QUESTIONS
## Does the order I filter in matter? If I pipe I'll get a different answer?
## What's up with 322-336 WI/ SW 3RD AVE? 215 sqft and sold for 3mil
  ## might be smart to have a $ per sqft dummy as well
#
## TASKS
## clean MKTVALYR3 so that all values have same format
## go through trust flag one by one and remove
## take out observation with string LLC and then get top 1%
## Year built make 9999 NA
## remove props with small totalarea -- some are like 200 sqft
## remove building type as garage? YES
## check dates are in order

# Compare SALEPRICE to TOTALVAL3 --- 
# note that this is the value of land + building
# MKTVALYR3 can take on values 09/27/17, 2017, or 2018
```


### Exploration

```{r, echo = FALSE}

# A Look at the Dataset

salma <- summary(dat1) # notes in thesis notebook

# Correlation -- Scatterplots

controls <- dat1 %>%
  dplyr::select(price_diff, pct_canopy_cov, taxlot_area, yearbuilt, totalsqft, dist_ugb, dist_cityhall, percent_vacant, CN_score, 
         SALEPRICE)
nm <- names(controls)

correlation <- cor(controls, use = "pairwise.complete.obs")
#takes only obs for which both vars are not NA


for (i in seq_along(nm)) {
  print(names(controls[i]))
  print(ggplot(controls, aes_string(y = nm[length(nm)], x = nm[i])) + 
    geom_point(alpha = 0.1) +
    geom_smooth())
}


ggplot(dat1, aes(y = SALEPRICE, x = TOTALVAL3)) + 
    geom_point()

ggplot(dat1, aes(y = log(SALEPRICE), x = taxlot_area)) + 
    geom_point()

# Outliers -- Box Plots
# best for discrete x

for (i in seq_along(nm)) {
  print(names(controls[i]))
  print(ggplot(controls, aes_string(y = nm[length(nm)], x = nm[i])) + 
    geom_boxplot(alpha = 0.1))
}

# Distribution of Constraints -- Density Plots
# Now to check the linearity of the variables it is a good practice to plot distribution graph and look for skewness of features. Kernel density estimate (kde) is a quite useful tool for plotting the shape of a distribution.

for (i in seq_along(nm)) {
  print(names(controls[i]))
  print(ggplot(controls, aes_string(x = nm[i])) + 
    geom_histogram(bins = 50))
}


```

SALE PRICE -- nicely right skewed with a peak around 400,000

Fireplaces -- having 2 fireplaces could tease out some info about median level houses

Full Baths -- as number of full baths increases, sale price increases. 

Taxlot Area -- a couple of outliers **i need to excise**. no trend with just sale price, need to try log sale price. **not sure how to interpret. ** right skewed but kinda ugly

Percent Canopy Cover -- useless blob. There's a concentration of data less than 50% canopy cover, but that might just be science. **not sure how to interpret. **
extremely right skewed

Price Difference -- perfectly linear, beautiful, im gonna cry. As the gap between sale price and assessed value becomes larger, sale price becomes larger. tells you absolutely nothing. more right skewed

Total Value Yr 3 -- roughly linear with a steep positive slope. 

**Year Built** -- not distinct, but older and newer houses tend to be more expensive

Total Sqft -- sorta linear, right skewed

**Distance to City Hall** -- super cool negative exponential trend!!!!! I'M SO EXCITED. As distance increases, sale price decreases at a decreasing rate. most people like 15,000-25,000 units (?) away from city centre

Percent Vacant -- nothing, surprisingly, weird peak at 100%

**Complete Neighborhoods Score** -- kind of an exponential? **need help** left skewed

Distance to UGB -- most people live 1000-3000 units (?) away from the UGB


A Look at the Dataset

number of rows, column, type of each control (struc, neighbor, etc), number of constraints, etc

edited: 3/5 12:07am
The cleaned dataset has

look at class to make sure everything's right
null/missing values vs zeros
means and medians, max and min for each var



### Regressions

```{r, echo = FALSE}

# Make a string of constraints and percent canopy names called "con_names"
clean <- SFR %>%
  dplyr::select(contains("con"), contains("pct")) %>%
  dplyr::select(-c("CONCRETE", "FIN SECOND", "PAVING/CONCRETE ONLY", "UNF SECOND", "conFldway", "pct_conFldway")) 
con_names <- names(clean)
con_names <- paste(con_names, collapse = " + ")

# mutate sale price transformations
dat1 %<>% 
  mutate(SALEPRICEbc = SALEPRICE^.46,
         SALEPRICElog = log(SALEPRICE),
         SALEPRICEsqrt = SALEPRICE^.5)



# LINEAR REGRESSION

# pre-analysis formula
sfr_lin = formula(paste0("SALEPRICE ~ 
                             f_baths + h_baths + n_fireplaces + yearbuilt + totalsqft + I(totalsqft^2) + taxlot_area + FLOORS + surfelev + percent_vacant + dist_cityhall + HIGH_SCH + MapLabel + CN_score + dist_ugb + ", con_names))

# regression
sfr_mod_linear <- lm(sfr_lin, dat1)
summary(sfr_mod_linear)
#not plotting observations with leverage one: 5135, 11888, 14466???


# BOX-COX (LINEAR) REGRESSION

plot(sfr_mod_linear)
bc = boxcox(sfr_mod_linear, lamba = seq(-3, 3, 1))
best_lam = bc$x[which(bc$y == max(bc$y))]

# pre-analysis formula
sfr_bc = formula(paste0("SALEPRICEbc ~ 
                             f_baths + h_baths + n_fireplaces + yearbuilt + totalsqft + I(totalsqft^2) + taxlot_area + FLOORS + surfelev + percent_vacant + dist_cityhall + HIGH_SCH + MapLabel + CN_score + dist_ugb + ", con_names))

# regression
sfr_mod_bc <- lm(sfr_bc, dat1)
summary(sfr_mod_bc)
plot(sfr_mod_bc)




# SEMI-LOG REGRESSION

#pre-analysis formula
sfr_log = formula(paste0("SALEPRICElog ~ 
                             f_baths + h_baths + n_fireplaces + yearbuilt + totalsqft + I(totalsqft^2) + taxlot_area + FLOORS + surfelev + percent_vacant + dist_cityhall + HIGH_SCH + MapLabel + CN_score + dist_ugb + ", con_names))

# regression
sfr_mod_log <- lm(sfr_log, dat1)
summary(sfr_mod_log)
plot(sfr_mod_log)





# # QUESTIONS
# # could switch zip code and neighborhood out, or take out county altogether
# # saledate?
# # high school or elementary school?
# # turn year built into age for better interpretation?
# # units of surfelev? "Average of the average surface elevation (above NAVD88 datum) of the land beneath and across all buildling footprints in feet"
#   
# # TASKS
# # check all variables for necessary transformations
# # combine GARAGE
# # create CAPACITY
# # create OVERSIZED LOT DUMMY
# # combine BASEMENT
# # combine ATTIC
# # SAT? Student exp?
# # transform zone????????????????

```
a little off at the edges
normality of errors is really bad, skewing all over the place
homoskedasticity is really bad too, should be a straight line but is not
residuals?? unsure how to interpret, theres a little red line

### Output

```{r}
stargazer(sfr_mod_linear, sfr_mod_log, sfr_mod_bc, title="Results", align=TRUE)


----
  
  
sfr_mod_linear <- tidy(lm(sfr_lin, dat1))
sfr_mod_log <- tidy(lm(sfr_log, dat1))
sfr_mod_bc <- tidy(lm(sfr_bc, dat1))

all_models <- rbind_list(
    sfr_mod_linear %>% mutate(model = 1),
    sfr_mod_log %>% mutate(model = 2),
    sfr_mod_bc %>% mutate(model = 3))

all_models

ols_table <- all_models %>%
    select(-statistic, -p.value) %>%
    mutate_each(funs(round(., 2)), -term) %>% 
    gather(key, value, estimate:std.error) %>%
    spread(model, value) 

ols_table


------------
  
tab_model(sfr_mod_linear, sfr_mod_log, sfr_mod_bc, file="try_1.doc")

```



### Diagnostics

```{r, include = FALSE}

# summary

# p-value

# t-value

# R^2 and Adj R^2

# SE

# F-Stat

# AIC and BIC


```


### Cross-Validation and Sensitivity Analysis

```{r, include = FALSE}
# Cross-validation. By splitting the data into multiple parts, we can check if an analysis (like a fitted model) based on one part of the data generalizes to another part of the data as well. Cross-validation is generally inappropriate, though, if there are correlations within the data, e.g. with panel data. Hence other methods of validation sometimes need to be used. For more on this topic, see statistical model validation.
# 
# Sensitivity analysis. A procedure to study the behavior of a system or model when global parameters are (systematically) varied. One way to do that is via bootstrapping.

```


```{r}

ass_val_zero <- SFR %>%
  filter(TOTALVAL3 == 0)

store <- dat1$SALEDATE
class(store)
unique <- unique(store)
(sort(unique))

dat2 <- dat1 %>%
  mutate(SITEZIP = as.factor(SITEZIP),
         MKTVALYR3 = case_when( MKTVALYR3 == "09/27/17" ~ "2017",
                                TRUE ~ "2018"),
         MKTVALYR3 = as.numeric(MKTVALYR3),
         SALEDATE = as.Date(SALEDATE, "%m/%d/%Y")
         )


filter(dat1, STATE_ID == "1S2E02AD  101")

# prpcd-desc-- where is vacant land?
# is there a problem with year built being a decimal?
```



```{r}
noncon <- thesis_data %>%
  filter(prop_type == "Non-conforming")
```




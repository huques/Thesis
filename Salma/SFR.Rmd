---
title: "Cleaning and Regressions"
output: pdf_document
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Import Packages + Read in Data

```{r, include = FALSE}
library(tidyverse)
library(magrittr)
library(here)
library(data.table)
library(MASS)
library(knitr)
library(xtable)
library(pander)
library(stargazer)
library(lubridate)

# read in csv
thesis_data <- read_csv(here::here("DATA","thesis-data.csv"))
```

### Cheatsheet
thesis_data <- raw data from csv into dataframe format

SFR <- all data where proprty type is single family residential

test1 <- creating flags for cleaning vars

dat1 <- applying flags to trim dataset

### Create SFR Dataframe

```{r, echo = FALSE}


# LOOKING AT PROPCD_DESC 
# problem: many of the fringe observation we have are not actually SFR, they include rowhouses and manufactured home parks and the like. Here, I am going to try to get to a better sample dataset of SFR. 


# First: Try filtering by PRPCD = Residential Improved instead of Prop_type = SFR
resimp <- thesis_data %>%
  filter(PRPCD_DESC == "RESIDENTIAL IMPROVED")

# problem -- there's commercial properties and all sorts of different sale_zones still in this:

#  [1] "Residential 5,000"              "Residential 2,000"             
#  [3] "Residential 2,500"              "Residential 1,000"             
#  [5] "Neighborhood Commercial 2"      "General Commercial"            
#  [7] "Residential 7,000"              "Residential 3,000"             
#  [9] "Storefront Commercial"          "Residential 10,000"            
# [11] "General Employment 2"           "Mixed Commercial / Residential"
# [13] "Residential Farm / Forest"      "High Density Residential"      
# [15] "Central Commercial"             "Residential 20,000"            
# [17] "Neighborhood Commercial 1"      "General Industrial 1"          
# [19] "Institutional Residential"      "Central Residential"           
# [21] "Office Commercial 2"            "Central Employment"            
# [23] "General Industrial 2"           "Office Commercial 1"           
# [25] "General Employment 1"           "Heavy Industrial"   

# Second: Try filtering by sale_zone in the base zoning for SFR. Note that it might still include duplexes and some attached family residences, but should still mainly be detached SFRs

reszone <- thesis_data %>%
  filter(sale_zone %in% c("RF", "R20", "R10", "R7", "R5", "R2.5"))

# this does same thing as prop_type = single family lol. you get all these gross PRPCD-DESCs:

#  [1] "RESIDENTIAL IMPROVED"              "2-4 UNIT MULTI-FAMILY"            
#  [3] "CHURCH"                            "VACANT LAND"                      
#  [5] "ROW/ATTACHED HOUSING"              "SPECIAL CARE/NURSING/SKILLED CARE"
#  [7] "ADULT FOSTER CARE"                 "STORE RETAIL-SMALL"               
#  [9] "9-20 UNIT MULTI-FAMILY"            "MISC IMPROVEMENTS"                
# [11] "5-8 UNIT MULTI-FAMILY"             "GENERIC COMMERCIAL USE"           
# [13] "CT APT 21-100 UNITS"               "RESTAURANT TAVERN"                
# [15] "OFFICE LOW RISE"                   "WHSE GENERAL/MISC"                
# [17] "MANUFACTURE HOME PARK"             "STOREFRONT"                       
# [19] "RESIDENTIAL, COMMERCIAL USE"       "WALK-UP APT 5-20 UNITS"           
# [21] "PUBLIC BLDG"                       "ASSISTED LIVING/INDEPENDENT"      
# [23] "STORE W/APT OR OFFICE OVER"        "DAY CARE CENTER"                  
# [25] "MIXED PRIMARY BUILDINGS"           "RESTAURANT DINING/LOUNGE"  


# Third: Combine first and second

SFR <- thesis_data %>%
  filter(PRPCD_DESC == "RESIDENTIAL IMPROVED", prop_type == "Single-family")

# gives 25,266 observations off the bat, meaning we lost 9,362 from thesis_data and 1,934 additional variables that are assigned "single family" as property type, but are zoned otherwise. For now, I am going to use this dataframe to run everything below. 

# Check for duplicates one more time
# All equal zero, meaning there are no duplicates of each of these variables
# Means we can use any one of them as a way of IDing observations uniquely
sum(data.frame(table(SFR$STATE_ID))$Freq > 1) 
sum(data.frame(table(SFR$RNO))$Freq > 1) 
sum(data.frame(table(SFR$PROPERTYID))$Freq > 1) 
sum(data.frame(table(SFR$TLID))$Freq > 1) 


# QUESTIONS
# What to do with non-conforming? Look up on PDX Maps
```

SFR data frame has 25,266 observations initially.

No observation repeat in any of the data frames acording to the unique IDs we have recorded for them. 


### Examining Noncomforming

```{r}
noncon <- thesis_data %>%
  filter(prop_type == "Non-conforming")
```


### Clean SFR Dataframe

```{r, echo = FALSE}
#-------------------- BEGINNING RYAN CODE (with minor edits)---------------------

test1 <- SFR %>%
  dplyr::select(-c(OWNER2, OWNER3, OWNERZIP, 
            MKTVALYR1, MKTVALYR2,
            BLDGVAL1, BLDGVAL2,
            LANDVAL1, LANDVAL2,  
            TOTALVAL1, TOTALVAL2,
            MS_GRADE, ES_GRADE,
            LEGAL_DESC, TAXCODE, PROP_CODE, LANDUSE, BEDROOMS, ACC_STATUS, NAME, COMMPLAN, SHARED, COALIT, HORZ_VERT, AUDIT_NBRH, MIDDLE_SCH,  Category, SOURCE, FRONTAGE, COUNTY, YEARBUILT, bldgtype)) %>%
  mutate(top_1 =  SALEPRICE > quantile(SALEPRICE, .99),
         MKTVALYR3 = case_when(MKTVALYR3 != 2018 ~ 2017, 
                                         TRUE ~ 2018),
         price_diff = SALEPRICE - LANDVAL3, 
         price_ratio = SALEPRICE/LANDVAL3 * 100,
         vacant_dummy = PRPCD_DESC == "VACANT LAND",
         llc_flag = grepl("LLC", OWNER1),
         proud_flag =  grepl("PROUD", OWNER1),
         trust_flag = grepl("TRUST", OWNER1) & !grepl("FAMILY", OWNER1) & !grepl("LIVING", OWNER1)) %>%
  mutate(arms_length = price_ratio > 20,
         na_if(yearbuilt, 0))

# switch the NAs in the constraints to 0s
to0 <- function(x){ifelse(is.na(x), 0, x)}

#------------------------- END RYAN CODE ----------------------------

# Make a string of constraints and percent canopy names called "con_names"
clean <- SFR %>%
  dplyr::select(contains("con"), contains("pct")) %>%
  dplyr::select(-c("CONCRETE", "FIN SECOND", "PAVING/CONCRETE ONLY", "UNF SECOND", "conFldway", "pct_conFldway")) 
clean_names <- names(clean)
con_names <- paste(clean_names, collapse = " + ")

# removing observations 
dat1 <- test1 %>% #25,266
  mutate_at(vars(clean_names), to0) %>%
  filter(top_1 == FALSE, #25,013, #253
         arms_length == TRUE, #24,495 #472
         vacant_dummy == FALSE, #24,495 #0
         proud_flag == FALSE, #24,480 #15
         llc_flag == FALSE, #24,128 #413
         trust_flag == FALSE #24,081 #52
  )

trust <- test1 %>%
  filter(top_1 == TRUE) %>%
  summarise(mean = mean(SALEPRICE),
            median = median(SALEPRICE))
#-------------------- BEGINNING RYAN CODE ------------------------

#garage sqft
gar_sqft_sum <- dat1 %>%
  dplyr::select(matches("gar")) %>%
  rowSums()

# attic sqft
attic_sqft_sum <- dat1 %>%
  dplyr::select(matches("attic")) %>%
  rowSums()

# basement sqft
bsmt_sqft_sum <- dat1 %>%
  dplyr::select(matches("bsmt")) %>%
  dplyr::select(-c("BSMT PARKING","BSMT GAR")) %>%
  rowSums()
  
dat1 %<>%
  mutate(garage_sqft = gar_sqft_sum,
         garage_dum = garage_sqft > 0,
         attic_sqft = attic_sqft_sum,
         attic_dum = attic_sqft > 0,
         bsmt_sqft = bsmt_sqft_sum,
         bsmt_dum = bsmt_sqft > 0,
         saledate = mdy(saledate), 
         year_sold = year(saledate)) %>%
  filter(between(totalsqft, 1, 7500), 
         yearbuilt > 1500)

#------------------------- END RYAN CODE ----------------------------

# test2 <- SFR %>%
#     as.Date(MKTVALYR, saledate)
# check if saledate is the same year as market year


 # dat1 %<>% filter(yearbuilt == 1008.000)
# deal with this


# ggplot(test1, aes(x = SALEPRICE, y = LANDVAL3)) +
#   geom_point() 
# 
# summary(test1$top_1)
# summary(test1$price_diff)
# summary(test1$price_ratio)
# summary(test1$vacant_dummy)
# summary(test1$arms_length)
# summary(test1$proud_flag)
# summary(test1$trust_flag)


## QUESTIONS
## Does the order I filter in matter? If I pipe I'll get a different answer?
## What's up with 322-336 WI/ SW 3RD AVE? 215 sqft and sold for 3mil
  ## might be smart to have a $ per sqft dummy as well
#
## TASKS
## remove props with small totalarea -- some are like 200 sqft

# Compare SALEPRICE to TOTALVAL3 --- 
# note that this is the value of land + building
# MKTVALYR3 can take on values 09/27/17, 2017, or 2018

# unique(dat1$X1)
# ncol(dat1)
# # for(i in 1:ncol(dat1)){
# 
# datcol <- colnames(dat1)
# 
# datcol
# 
# kelly <- matrix(data = NA, nrow = 40, ncol = length(dat1)) %>%
#   as.data.frame() %>%
#   rename(paste(datcol,))
# 
# for(i in seq_along(names(dat1))){
#   print(paste("hi"))
#   print(unique(dat1[i])
#   head(sort(unique(dat1$AREA)))
# }

```


### Exploration

```{r, echo = FALSE}

# A Look at the Dataset

salma <- summary(dat1) # notes in thesis notebook

# Correlation -- Scatterplots

controls <- dat1 %>%
  dplyr::select(price_diff, pct_canopy_cov, taxlot_area, yearbuilt, totalsqft, dist_ugb, dist_cityhall, percent_vacant, CN_score, 
         SALEPRICE)
nm <- names(controls)

correlation <- cor(controls, use = "pairwise.complete.obs")
#takes only obs for which both vars are not NA


for (i in seq_along(nm)) {
  print(names(controls[i]))
  print(ggplot(controls, aes_string(y = nm[length(nm)], x = nm[i])) + 
    geom_point(alpha = 0.1) +
    geom_smooth())
}


ggplot(dat1, aes(y = SALEPRICE, x = TOTALVAL3)) + 
    geom_point()

ggplot(dat1, aes(y = log(SALEPRICE), x = taxlot_area)) + 
    geom_point()

# Outliers -- Box Plots

# best for discrete x

for (i in seq_along(nm)) {
  print(names(controls[i]))
  print(ggplot(controls, aes_string(y = nm[length(nm)], x = nm[i])) + 
    geom_boxplot(alpha = 0.1))
}

# Distribution of Constraints -- Density Plots

# Now to check the linearity of the variables it is a good practice to plot distribution graph and look for skewness of features. Kernel density estimate (kde) is a quite useful tool for plotting the shape of a distribution.

for (i in seq_along(nm)) {
  print(names(controls[i]))
  print(ggplot(controls, aes_string(x = nm[i])) + 
    geom_histogram(bins = 50))
}


```


look at class to make sure everything's right
null/missing values vs zeros


### Regressions

```{r, echo = FALSE}

# mutate sale price transformations
dat1 %<>% 
  mutate(SALEPRICEbc = SALEPRICE^.46,
         SALEPRICElog = log(SALEPRICE),
         SALEPRICEsqrt = SALEPRICE^.5)

# LINEAR REGRESSION

# formula
sfr_lin = formula(paste0("SALEPRICE ~ 
                             f_baths + h_baths + n_fireplaces + yearbuilt + totalsqft + I(totalsqft^2) + taxlot_area + FLOORS + surfelev + percent_vacant + dist_cityhall + HIGH_SCH + MapLabel + CN_score + dist_ugb + ", con_names))

# reg
sfr_mod_linear <- lm(sfr_lin, dat1)
summary(sfr_mod_linear)

# output
sfr.table <- xtable(sfr_mod_linear)
kable(sfr.table, caption = "Linear table")


star.1 <- stargazer(sfr_mod_linear,
                    title="Title: Regression Results"
                    )
kable(star.1, caption = "Linear table stargazr")

pander(summary(sfr_mod_linear)) 
#not plotting observations with leverage one: 5135, 11888, 14466???

# output v4 : turns into html -- GOOD
# library(stargazer)
# stargazer(sfr_mod_linear, type="html", out="models.htm")

# output v2 : creates a data frame that can also be converted into word
# library(broom)
# will <- dat1 %>% 
#   do(tidy(lm(sfr_lin, data = .)))

#output v3 : creates a .txt file with stars! difficult to parse properly -- not good
# sink("lm.txt")
# print(summary(sfr_mod_linear, signif.stars = TRUE))
# sink()

# output v5 : turns into csv -- ugly -- not good
# library(broom)
# write.csv(tidy(sfr_mod_linear) , "hmm.csv" )


# BOX-COX (LINEAR) REGRESSION

plot(sfr_mod_linear)
bc = boxcox(sfr_mod_linear, lamba = seq(-3, 3, 1))
best_lam = bc$x[which(bc$y == max(bc$y))]

# pre-analysis formula
sfr_bc = formula(paste0("SALEPRICEbc ~ 
                             f_baths + h_baths + n_fireplaces + yearbuilt + totalsqft + I(totalsqft^2) + taxlot_area + FLOORS + surfelev + percent_vacant + dist_cityhall + HIGH_SCH + MapLabel + CN_score + dist_ugb + ", con_names))

# regression
sfr_mod_bc <- lm(sfr_bc, dat1)
summary(sfr_mod_bc)
plot(sfr_mod_bc)




# SEMI-LOG REGRESSION

#pre-analysis formula
sfr_log = formula(paste0("SALEPRICElog ~ 
                             f_baths + h_baths + n_fireplaces + yearbuilt + totalsqft + I(totalsqft^2) + taxlot_area + FLOORS + surfelev + percent_vacant + dist_cityhall + HIGH_SCH + MapLabel + CN_score + dist_ugb + ", con_names))

# regression
sfr_mod_log <- lm(sfr_log, dat1)
summary(sfr_mod_log)
plot(sfr_mod_log)

# output v1 : creates an xable that can be converted in word, no *** or stats
sfr_lin.table <- xtable(sfr_lin_mod)
kable(sfr_lin.table, caption = "SFR Linear table dummies")
print.xtable(sfr_lin.table, type="html", file="sfr_lin_xtable.html")

sfr_bc.table <- xtable(sfr_mod_bc)
kable(sfr_bc.table, caption = "SFR Box-Cox Regression Results")
print.xtable(sfr_bc.table, type="html", file="sfr_bc_xtable.html")

sfr_log.table <- xtable(sfr_mod_log)
kable(sfr_log.table, caption = "Linear table")
print.xtable(sfr_log.table, type="html", file="sfr_log_xtable.html")


# # QUESTIONS
# # could switch zip code and neighborhood out, or take out county altogether
# # saledate?
# # high school or elementary school?
# # turn year built into age for better interpretation?
# # units of surfelev? "Average of the average surface elevation (above NAVD88 datum) of the land beneath and across all buildling footprints in feet"
#   
# # TASKS
# # check all variables for necessary transformations
# # create CAPACITY
# # create OVERSIZED LOT DUMMY
# # SAT? Student exp?
# # transform zone????????????????

```
a little off at the edges
normality of errors is really bad, skewing all over the place
homoskedasticity is really bad too, should be a straight line but is not
residuals?? unsure how to interpret, theres a little red line

### Output

# ```{r}
# stargazer(sfr_mod_linear, sfr_mod_log, sfr_mod_bc, title="Results", align=TRUE)
# 
# 
# ----
#   
#   
# sfr_mod_linear <- tidy(lm(sfr_lin, dat1))
# sfr_mod_log <- tidy(lm(sfr_log, dat1))
# sfr_mod_bc <- tidy(lm(sfr_bc, dat1))
# 
# all_models <- rbind_list(
#     sfr_mod_linear %>% mutate(model = 1),
#     sfr_mod_log %>% mutate(model = 2),
#     sfr_mod_bc %>% mutate(model = 3))
# 
# all_models
# 
# ols_table <- all_models %>%
#     select(-statistic, -p.value) %>%
#     mutate_each(funs(round(., 2)), -term) %>% 
#     gather(key, value, estimate:std.error) %>%
#     spread(model, value) 
# 
# ols_table
# 
# 
# ------------
#   
# tab_model(sfr_mod_linear, sfr_mod_log, sfr_mod_bc, file="try_1.doc")

```



### Diagnostics

```{r, include = FALSE}

# summary

# p-value

# t-value

# R^2 and Adj R^2

# SE

# F-Stat

# AIC and BIC


```


### Cross-Validation and Sensitivity Analysis

```{r, include = FALSE}
# Cross-validation. By splitting the data into multiple parts, we can check if an analysis (like a fitted model) based on one part of the data generalizes to another part of the data as well. Cross-validation is generally inappropriate, though, if there are correlations within the data, e.g. with panel data. Hence other methods of validation sometimes need to be used. For more on this topic, see statistical model validation.
# 
# Sensitivity analysis. A procedure to study the behavior of a system or model when global parameters are (systematically) varied. One way to do that is via bootstrapping.

```


```{r}

ass_val_zero <- SFR %>%
  filter(TOTALVAL3 == 0)

store <- dat1$SALEDATE
class(store)
unique <- unique(store)
(sort(unique))

dat2 <- dat1 %>%
  mutate(SITEZIP = as.factor(SITEZIP),
         MKTVALYR3 = case_when( MKTVALYR3 == "09/27/17" ~ "2017",
                                TRUE ~ "2018"),
         MKTVALYR3 = as.numeric(MKTVALYR3),
         SALEDATE = as.Date(SALEDATE, "%m/%d/%Y")
         )


filter(dat1, STATE_ID == "1S2E02AD  101")

# prpcd-desc-- where is vacant land?
# is there a problem with year built being a decimal?
```



PLAYING AROUND WITH PRICE RATIO
PLAYING AROUND WITH IMPROVMENT DUMMY VS IMPROVEMENT SQFT
PLAYING AROUND WITH CONSTRAINT DUMMY VS INDIVUDAL DUMMY FOR EACH CONSTRAINT
PLAYING AROUND WITH CN_SCORE VS NEIGHBORHOOD VS ELEMENTARY SCHOOL (bc its included within cn_score)
PLAYING AROUND WITH KEEPING IN TRUSTS AND LLCs
PLAYING AROUND WITH VACANT DUMMY VS PERCENT VACANT (AND OTHER PERCENT CONSTRAINTS)
PLAYING AROUND WITH ZONING HISTORY -- ?????
PLAYING 





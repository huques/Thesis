---
title: "Construct Full Data Frame 2"
output: pdf_document
---

### 1. Load packages
```{r}
library(tidyverse)
library(sf)
library(sp)
library(stringr)
library(data.table)
library(magrittr)
library(readxl)

datanames <- c("gisimpcop", "impsegcop", "gispropcop", "allpropcop", "segchar", "rollhist", "impseg", "salescop","school", "imps", "constraints", "rollhist_wide", "taxlots", "footprints")
datanames

# Set working directory
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
```



### 2. Set pathnames and load databases
```{r}
#----------------------
# GDB sent 10/29 via email
gdb <- "./DATA/data2.gdb"

capacity <- st_read(gdb, layer = "bli_development_capacity")
impsegcop <- st_read(gdb, layer = "CoP_OrionImprovementSegment")
gisimpcop <- st_read(gdb, layer = "CoP_GISImprovement")
impseg <- st_read(gdb, layer = "imp_segments")
school <- st_read(gdb, layer = "school_attendance_areas")

#----------------------
# GDB sent 10/10 via USB
gdb2 <- "./DATA/data1.gdb"

taxlots <- st_read(gdb2, "taxlots_20191010")
footprints <- st_read(gdb2, "building_footprints_20191010")

#----------------------
# GDB sent 11/12 via email
gdb3 <- "./DATA/data_20191112.gdb"

bli_constraints <- st_read(gdb3, "bli_constraints_all")
nbhd <- st_read(gdb3, "neighborhoods_no_overlap")

#----------------------
# GDB sent 11/22 via email
gdb4 <- "./DATA/tree_canopy.gdb"

bli_constraints_v2 <- st_read(gdb4, "bli_constraints_v2_pts_run4")

#----------------------
# SHP sent 11/25 via email
ugb <- st_read("./DATA/ugb/ugb.shp") 


#----------------------
# GDB sent 2/10 via email
gdb5 <- "./DATA/canopy_20200210.gdb"

canopy <- st_read(gdb5, "canopy_taxlot_intersect")

#----------------------
# GDB 2/11 from PortlandMaps
walk_gdb <- "./DATA/Complete_Neighborhoods_Scoring_Surface"

walk <- st_read(walk_gdb, 
           layer = "Complete_Neighborhoods_Scoring_Surface")

```



### Take sample of taxlots
```{r}
begin <- as.Date("2015-01-01")
end <- as.Date("2019-01-01")

# Reformat SALEDATE column (save as date type so that we can
# do math on it)
taxlots$saledate <- as.Date(as.character(taxlots$SALEDATE), "%m/%d/%Y")

# Take time interval of taxlots we're interested in (5-years)
taxlots_pruned <- taxlots %>%
  filter(saledate > begin & saledate < end)

# Grab keys to the lots we want for state and property ids
stateids <- taxlots_pruned %>%
  pull(STATE_ID)
propids <- taxlots_pruned %>%
  pull(PROPERTYID)


#-------------------------------------------------------------------------------
# YAY! They're sums below are both zero, meaning we have 1:1 keys!
# sum(data.frame(table(taxlots_pruned$STATE_ID))$Freq > 1)
# sum(data.frame(table(taxlots_pruned$PROPERTYID))$Freq > 1)

```


### Complete Neighborhood Walkscores!!!

```{r}
# Transform crs from 4326 to 2913
walk %<>% 
  st_transform(2913) %>%
  select(CN_score) 
# dropped all other variables (like shape area) to avoid
# renaming things and later confusion
# Join!
fugly <- sf::st_join(st_centroid(taxlots_pruned), walk)
print(paste0("walkscore Join: ", as.character(sum(data.frame(table(fugly$STATE_ID))$Freq > 1))))

```

### Neighborhood Fixed Effects

```{r}

nbhd %<>% 
  dplyr::select(-c(OBJECTID, Shape_Leng, Shape_Length, Shape_Area))

fugly <- st_join(st_centroid(fugly), nbhd)
print(paste0("nbhd Join: ", as.character(sum(data.frame(table(fugly$STATE_ID))$Freq > 1))))

#-------------------------------------------------------------------------------------------------------------
# 
# # Bring in neighborhood fixed effects
# df <- st_intersection(taxlots_pruned, nbhd)
# 
# # Problem: there are 567 observations where STATE_IDs are double counted.
# nbhd.overlap <- df %>%
#   arrange(STATE_ID) %>%
#   dplyr::group_by(STATE_ID) %>%
#   dplyr::mutate(n = n()) %>%
#   dplyr::filter(n > 1 )
# 
# # Calculate intersection areas. 
# # Notice that new geometries were generated from the st_intersection() function
# # used to create `df`. These geometries are the intersections between the 99 neighborhoods
# # and our 34628 taxlots. A STATE_ID appears in `df` multiple times when a single taxlot 
# # intersects more than one neighborhood. 
# 
# #------------------ FIX MULTI STATE_ID DOUBLE COUNT -----------------------
# 
# # I place  a double-counted taxlot in the neighborhood that intersects a plurality of 
# # its total area.
# 
# # Add area variable
# nbhd.overlap$area.nbhd <- st_area(st_geometry(nbhd.overlap))
# 
# # Groupby STATE_ID and add a variable that is the maximum of the double counted taxlots' areas
# nbhd.overlap %<>%
#   group_by(STATE_ID) %>%
#   mutate(max = max(area.nbhd)) 
# 
# nbhd.big <- nbhd.overlap %>%
#   filter(as.numeric(max) == as.numeric(area.nbhd)) 
# 
# # DO NOT NEED TO DEFINE UNLESS WE NEED THE GEOMETRIES FOR SOME REASON
# nbhd.small <- nbhd.overlap %>%
#   filter(as.numeric(max) != as.numeric(area.nbhd)) 
# # since nbhd.small has more rows than nbhd.big, we know that some taxlots
# # intersect more than 2 neighborhoods.
# 
# nbhd.single <- df %>%
#   arrange(STATE_ID) %>%
#   dplyr::group_by(STATE_ID) %>%
#   dplyr::mutate(n = n()) %>%
#   dplyr::filter(n == 1)
# 
# #-----------------------------------------
# # NOT NECESSARY!
# # This loop combines the geometries from where the STATE_IDs had split & replaces partial geometries
# # in nbhd.big with complete geometries.
# #for(r in 1:nrow(nbhd.big)){
# #  id <- nbhd.big[r,]$STATE_ID
# #  tinydf <- nbhd.small %>%
# #    filter(STATE_ID == id)
# #  st_geometry(nbhd.big[r,]) <- st_combine(st_geometry(tinydf))
# #}
# #-----------------------------------------
# 
# nbhd.single$area.nbhd <- NA # define these columns to ensure rbind() works properly.
# nbhd.single$max <- NA
# df <- rbind(nbhd.single, nbhd.big) # correct  number of dimensions!
# 
# dim(df) # note that the dimension of the newly created df is smaller than the taxlots, meaning there 
# # are some properties in which there was no corresponding neighborhood (this introduces selection
# # bias). 
# 
# # RESOLVE SELECTION BIAS:
# # list of STATE_IDS that were dropped
# diff <- setdiff(unique(taxlots_pruned$STATE_ID), unique(df$STATE_ID))
# 
# # Rename ambiguous nbhd geometry columns 
# df %<>%
#   rename(Shape_Leng.nbhd = Shape_Leng,
#          Shape_Length.nbhd = Shape_Length.1,
#          Shape_Area.nbhd = Shape_Area.1,
#          area.nbhd.insx = area.nbhd,
#          NBHD_NAME = NAME)
# 
# # grab the lots for which no nbhd was recorded (those that went missing in the intersection)
# # initialize vars that were added in the intersection (so rbind can join properly)
# removed_lots <- taxlots_pruned %>%
#   filter(STATE_ID %in% diff) %>%
#   mutate(Shape_Leng.nbhd = NA,
#          Shape_Length.nbhd = NA,
#          Shape_Area.nbhd = NA,
#          area.nbhd.insx = NA, 
#          NBHD_NAME = NA,
#          MapLabel = NA,
#          COMMPLAN = NA,
#          SHARED = NA,
#          COALIT = NA,
#          HORZ_VERT = NA,
#          NBRNUM = NA,
#          AUDIT_NBRH = NA,
#          OBJECTID = NA,
#          n = NA,
#          max = NA)
#          
# # binding retains the lots that had dropped out!
# df <- rbind(df, removed_lots)
```


### Percent Vacant Properties within Half Mile *correction-200 ft*

```{r}
# selects property id's and vacancy variable
# 1 ft = 0.3048 meters
conv <- 0.3048
ft <- 200
buffer_dist <- ft * conv

vacancy <- capacity %>%
  select(STATE_ID, IS_VACANT, Shape) %>%
  rename(ShapeCapacity = Shape)

vacancy_pruned <- vacancy %>%
  group_by(STATE_ID) %>%
  slice(1) %>%
  mutate(VACANT=recode(IS_VACANT, 
                       "True" = 1,
                       ` `= 0)) %>%
  select(-IS_VACANT)


# creates a buffer called buffer_dist (measured in meters)
# adds this buffer as an additional geometry to the main dataset
buffy <- fugly %>%
  rename(ShapeBuffy = Shape) %>%
  st_buffer(buffer_dist)

# join buffy and vacancy_pruned 
vacant_join_buffy <- st_join(buffy, vacancy_pruned, left = TRUE) # returns all buffy

# calculates percent vacant houses in buffer
vacant_var <- vacant_join_buffy %>%
  st_drop_geometry() %>%
  arrange(STATE_ID.x) %>%
  group_by(STATE_ID.x) %>%
  rename(STATE_ID = STATE_ID.x) %>%
  mutate(n = n()) %>%
  mutate(VACANT = recode(VACANT, 
                       "1" = 1,
                       "0" = 0)) %>%
  summarize(percent_vacant = sum(VACANT)/n[1])

# join to dataframe 
fugly <- left_join(fugly, vacant_var, by = "STATE_ID")
print(paste0("vacant Join: ", as.character(sum(data.frame(table(fugly$STATE_ID))$Freq > 1))))

```

### St_intersection school to taxlots

One problem with an st_intersection is some lots are divided by the school catchment area boundaries, resulting in non-unique STATE_IDs. The st_intersection adds a second observation if it a tax lot is partially within catchment area 1 and partially within catchment area 2.

This is the reason the number of observations rose after the intersection. 

```{r}

school %<>% 
  dplyr::select(-c(Shape_Leng, Shape_Length, Shape_Area))

fugly <- st_join(st_centroid(fugly), school)
print(paste0("school Join: ", as.character(sum(data.frame(table(fugly$STATE_ID))$Freq > 1))))

# -----------------------------------------------------------------------------------------------------------
# 
# # Naive st_intersection 
# testjoint3 <- st_intersection(fugly, school)
# 
# # Save multiple STATE_ID obs in `schzone`
# schzone <- testjoint3 %>%
#   group_by(STATE_ID) %>%
#   mutate(n = n()) %>%
#   filter(n > 1)
# 
# # We can see that adding the number of unique stateIDs that are observed more than once
# # to the old dimensions is equal to the number of rows in testjoint3.
# nrow(m2) + length(unique(schzone$STATE_ID)) == nrow(testjoint3)
# 
# # Fix: choose a catchment area for multiple STATE_ID lots depicted below. 
# schzone %>%
#   arrange(STATE_ID) %>%
#   select(contains("SCH"), Shape_Area, Shape_Area.1) %>%
#   st_drop_geometry() %>%
#   head() %>%
#   kable() %>% 
#   kable_styling(full_width = F)
# 
# 
# # Choose based on percentage of lot within that catchment area
# schzone$area.school <- st_area(st_geometry(schzone))
# testjoint3$area.school <- st_area(st_geometry(testjoint3))
# 
# schzone %<>%
#   group_by(STATE_ID) %>%
#   mutate(max = max(area.school)) 
# 
# schzone.big <- schzone %>% # add containing containing max area by STATE_ID
#   filter(as.numeric(max) == as.numeric(area.school)) 
# 
# schzone.small <- schzone %>%
#   filter(as.numeric(max) != as.numeric(area.school)) 
# 
# schzone.big$STATE_ID == schzone.small$STATE_ID # this vector should all be true
# 
# # Use st_combine to meld the broken boundaries from schzone.small and schzone.big geometry sets.
# # note: I did not use st_union(st_geometry(schzone.small), st_geometry(schzone.big)) because
# # st_union() takes the cartesian union of both giving a much larger feature set than neccessary, 
# # we only want the diagonals
# for(r in 1:nrow(schzone.big)){
#   st_geometry(schzone.big[r,]) <- st_combine(c(st_geometry(schzone.big)[r], 
#                st_geometry(schzone.small)[r]))
# }
# 
# # Now, combine the fixed geometries and catchment areas with the single-observation
# # subset of `testjoint3`
# single.school <- testjoint3 %>%
#   group_by(STATE_ID) %>%
#   mutate(n = n(),
#          max = NA) %>%
#   filter(n == 1) # This is the right number of rows, 34426
# 
# # Row number check
# nrow(schzone.big) + nrow(single.school) == nrow(taxlots_pruned)
# 
# testjoint3 <- rbind(schzone.big, single.school)
```


### Generate distance to city hall

```{r}
# grab lat long from google maps
cityhall <- data.frame(place = "City Hall",
                       long = -122.679103,
                       lat = 45.515000)
# reformat from df to sf object
cityhall <- st_as_sf(cityhall, coords = c("long", "lat"))

# Set coordinate system
cityhall <- cityhall %>%
  st_set_crs(4326) %>%
  st_transform(2913)

# HERE IS THE ERROR
fugly$dist_cityhall <- st_distance(cityhall, fugly, which = "Euclidean")[1,]
```


### Distance to Urban Growth Boundary!!!!

This approach calculates distances using the centroids of MULTIPOLYGON taxlots. So, in the process, the st_geometry() attribute of the taxlot spatial data frame is changed from polygons to points. But the geometry can be reset to MULTIPOLYGONS afterward.
```{r}

# Transform UGB shapefile
ugb %<>% 
  st_transform(2913) %>% 
  st_cast(., "MULTILINESTRING")

# Convert to sp object since we use the rgeos::NearestPoints function that requires this
# data type
ugb.sp <- as_Spatial(ugb)


# Pull taxlot polygons & calculate centroids
centroids <- st_geometry(fugly)
centroids.sp <- as_Spatial(centroids)

# gNearestPoints returns vector of nearest point on taxlot (the centroid) 
# and the nearest point on the ugb. We take the ugb point, the second item in the list.

# Initialize empty list
nearest_points <- list(NA, nrow(fugly))

# call gNearestPoints over all observations in `taxlots` to return list of point geometries
for(i in 1:nrow(fugly)){
  nearest_points[i] <- st_as_sf(rgeos::gNearestPoints(centroids.sp[i,], ugb.sp)[2,])
}

# Combine/"unlist" the point geometries
nearest_points <- do.call(c, nearest_points)

# When given two vectors, st_distance(a, b) returns distance between all pairs of points in a and a.
# ex. if a = c(1,2,3) b = c(0, 9, 8). then st_distance(a, b) = (1, 8, 7, 2, 7, 6, 3, 6, 5)
# mapply() loops over nearest_points and centroids simultaneously so usign the above a, b, 
# we end up with (1,7,5)
dist_to_ugb <- mapply(FUN = st_distance, nearest_points, centroids)


fugly$dist_ugb <- dist_to_ugb

```




### Collapse footprints

Now we will try merging/collapsing footprints on this smaller subset of the entire data set, only homes/MFR that have sold in the last 5 years.

```{R}
# grab only the footprints associated with stateids in our sample
ftprints_pruned <- footprints %>%
  filter(STATE_ID %in% stateids)

# Make collapsed footprints dataset called `allfeet`
ftprints_pruned %<>% 
  st_drop_geometry() %>%
  dplyr::group_by(STATE_ID) %>%
  dplyr::summarise(totalsqft = sum(BLDG_SQFT, na.rm = T),
            yearbuilt = mean(YEAR_BUILT, na.rm = T),
            avgheight = mean(AVG_HEIGHT, na.rm = T),
            surfelev = mean(SURF_ELEV, na.rm = T),
            minheight = mean(MIN_HEIGHT, na.rm = T),
            maxheight = mean(MAX_HEIGHT, na.rm = T),
            volume = mean(VOLUME, na.rm = T),
            bldgtype = BLDG_TYPE[1],
            bldguse = BLDG_USE[1]) 

# Left join ftprints_pruned to fugly
fugly <- left_join(fugly, ftprints_pruned, 
               by = "STATE_ID")
print(paste0("feet Join: ", as.character(sum(data.frame(table(fugly$STATE_ID))$Freq > 1))))

#-------------------------------------------------------------------------------
# # check if these ids uniquely identify the building footprints
# multi_foot <- ftprints_pruned %>%
#   lwgeom::st_make_valid() %>%
#   group_by(STATE_ID) %>%
#   mutate(n = n()) %>%
#   filter(n > 1)
# 
# dim(multi_foot) # they do not

```


### Prune and reshape impsegcop
Impsegcop has our attached garage, patio, basement, attic, deck, variables among others. Reshape to get the square footage of each segment type. For properties that have multiple decks, attics, etc, the square footage was summed by type, so currently there is no way to distinguish between properties that have multiple attics or basements. We can think about whether proxying with a summed square footage is actually reasonable. 
```{r}
impsegcop$SegmentSqFt <- as.numeric(impsegcop$SegmentSqFt) # so we can eventually sum

isc_pruned <- impsegcop %>%
  filter(PropID %in% propids)

# Second attempt to widen using dcast: works perfectly on 1:1 data
# Reshape using dcast. Set fun.aggregate to sum, which means that we
# sum the square footage across multiple obs.
impsegcop_wide <- dcast(setDT(isc_pruned), PropID ~ SegmentType,
              value.var = c("SegmentSqFt"),
              fill = 0,
              fun.aggregate = sum)

# Rename Pproperty ID so that we can join to fugly
impsegcop_wide %<>%
  dplyr::rename(PROPERTYID = PropID)

# Left join impsegcop_wide to fugly
fugly <- left_join(fugly, impsegcop_wide, 
               by = "PROPERTYID")
print(paste0("fugly impsegcop Join: ", as.character(sum(data.frame(table(fugly$STATE_ID))$Freq > 1))))

#-------------------------------------------------------------------------------
# dim(isc_pruned)
# sum(data.frame(table(isc_pruned$PropID))$Freq > 1)
# nrow(multi_isc)
# # check if propids ids uniquely identify the building footprints
# multi_isc <- isc_pruned %>%
#   group_by(PropID, SegmentType) %>%
#   mutate(n = n()) %>%
#   filter(n == 1)
# Code below turns 0 meaning PropID uniquely identifies observations
# sum(data.frame(table(impsegcop_wide$PropID))$Freq > 1)

```


### Prune and Reshape Impseg
We need the columns PlumbingCode and Fire_place_Lookup from the impseg df. However, it appears that there are other value variables such as perimeter, neighborhood marketvalue, total area, etc. that could be useful for our analysis. 
```{r}
# change from factor to numeric
is_pruned <- impseg %>%
  filter(PropertyID %in% propids)

#--------------------------------------------
# Using base r because dplyr code took too long

# Take only observations for which Plumbing_Code is non-missing
bath <- is_pruned[!is.na(is_pruned$Plumbing_Code),]
bath %<>% 
  dplyr::select(PropertyID, Plumbing_Code, Seg_Type) %>%
  dplyr::group_by(PropertyID) %>%
  dplyr::summarise(bath = paste0(Plumbing_Code, collapse = "|")) %>%
  dplyr::rename(PROPERTYID = PropertyID)

fugly <- left_join(fugly, bath, by = "PROPERTYID")
print(paste0("fugly bath Join: ", as.character(sum(data.frame(table(fugly$STATE_ID))$Freq > 1))))

# Recode bathrooms 
# Initialize columns: f.baths = total full baths, h.baths = total half baths
fugly$f_baths <- NA
fugly$h_baths <- NA

extractBaths <- function(string){
  if(is.na(string)){c(fb = NA, hb = NA)}
  else{
    str <- strsplit(string, "[[:punct:]]")[[1]]
    str <- sort(str)
    fb <- str[grepl("FB", str)]
    hb <- str[grepl("HB", str)]
    nfb <- sum(as.numeric(gsub("FB", "", fb)))
    nhb <- sum(as.numeric(gsub("HB", "", hb)))
    c(fb = nfb, hb = nhb)
  }
}

# Create 2 x 34480 matrix of number of bathrooms
bath_matrix <- sapply(fugly$bath, extractBaths)
fugly$f_baths <- bath_matrix[1,]
fugly$h_baths <- bath_matrix[2,]


# ---------------------------------
# Which segment types contain the fireplace code?
# `fire` is a data frame of all nonmissing Fire_Place_Code observations
fire <- is_pruned[!is.na(is_pruned$Fire_Place_Code),]

fire %<>% 
  dplyr::select(PropertyID, Fire_Place_Code, Seg_Type) %>%
  dplyr::group_by(PropertyID) %>%
  dplyr::summarise(fireplace = paste0(unique(Fire_Place_Code), collapse = "|")) %>%
  dplyr::rename(PROPERTYID = PropertyID)

fugly <- left_join(fugly, fire, by = "PROPERTYID") 
print(paste0("Fugly fire Join: ", as.character(sum(data.frame(table(fugly$STATE_ID))$Freq > 1))))


# Recode fireplaces

# I chose to treat all the fireplaces as the same and count the number of fireplaces/hearth
# That is fireplaces coded as MD2 = MODULAR 2 and BK1 = BRICK 1.
# Did this because it seems reasonable to think that differing styles/types of fireplaces
# won't have a measurable effect on property values. Can come back 
# to this decision, & check out the firelu table. 

extractHearth <- function(string){
  if(is.na(string)){NA}
  else{
    str <- strsplit(string, "[[:punct:]]")[[1]]
    length(str)
  }
}

fugly$n_fireplaces <- sapply(fugly$fireplace, extractHearth)


```






### Accessory Dwelling Unit Dummy

Not specified based on number of ADU's on a property. 

```{r}
# column (267352 obs) with unique PropID and ADU dummy
ADU <- gisimpcop %>%
  select(ImpType, PropID) %>%
  count(PropID) %>%
  mutate(ADUdummy = ifelse(n == 1, "0", "1")) %>%
  select(PropID, ADUdummy) %>%
  rename(PROPERTYID = PropID)

ADU <- ADU[-c(1),]

# join to dataframe 
fugly <- left_join(fugly, ADU, by = "PROPERTYID")
print(paste0("ADU Join: ", as.character(sum(data.frame(table(fugly$STATE_ID))$Freq > 1))))

```



### Percent of Lot Covered by Canopy

```{r}
# create new data frame in order to calculate total taxlot area for our "percentage of lot 
# covered by canopy" variable
taxlot_areas <- fugly %>%
  rename(taxlot_area = Shape_Area) %>%
  select(STATE_ID, taxlot_area) %>% 
  st_drop_geometry()

# This chunk collapses canopy by STATE_ID and generates canopy coverage ratios
canopy2 <- canopy %>%
  filter(STATE_ID %in% stateids) %>% # filter for just those STATE_IDs in the taxlots pruned data set
  rename(canopy_area = Shape_Area) %>%
  left_join(taxlot_areas, by = "STATE_ID") %>% # join the taxlot areas by state_id (for the denominator)
  st_drop_geometry() %>% # dropped because we had many self-intersections when trying to summarize
  group_by(STATE_ID) %>% 
  summarise(pct_canopy_cov = sum(canopy_area) / taxlot_area[1], # take the first element in taxlot_area
            total_canopy_cov = sum(canopy_area), # save numerator and denominator separately
            taxlot_area = taxlot_area[1]) 

# should end up with 33387 observations

fugly <- left_join(fugly, canopy2, by = "STATE_ID")
print(paste0("Canopy Join: ", as.character(sum(data.frame(table(fugly$STATE_ID))$Freq > 1))))
```


### Constraints!!!

```{r}
# Define dplyr helper functions
genDummy <- function(x){ifelse(is.na(x), 0, 1)}
setToOne <- function(x){ifelse(x > 1, 1, x)}

# Change <NA>, "True" instances to 0, 1 for all 204,000+ properties
non_id_constraints <- bli_constraints_v2 %>%
  select(contains("con")) %>%
  mutate_if(is.factor, genDummy)

# This join increases # of observations because sometimes there are several observations
# in non_id_constraints that are within the confines of a single taxlot polygon
firstjoin <- st_join(taxlots_pruned, non_id_constraints, left = TRUE)

# Collapse these observations so that we keep all the constraints as either 1 or 0 for each 
# property. Here, we're not worrying about partial constraints yet.
dumdum_constraints <- firstjoin %>%
  group_by(STATE_ID) %>%
  select(contains("con")) %>% 
  st_drop_geometry() %>%
  summarise_all(sum) %>%
  mutate_if(is.numeric, setToOne)

fugly <- left_join(fugly, dumdum_constraints, by = "STATE_ID")
print(paste0("constraints Join: ", as.character(sum(data.frame(table(fugly$STATE_ID))$Freq > 1))))

```

### Partial Constraints!

```{r}
# Read
intersectionz <- st_read("./DATA/constraint_layers.gdb", "Intersectionz")

partial <- intersectionz %>%
  filter(STATE_ID %in% stateids) %>%
  group_by(STATE_ID) %>%
  mutate(pct_conFld100 = ifelse(conFld100 == "True", 
                                sum(Shape_Area, na.rm = T) / AREA[1], 0),
         pct_conFldway = ifelse(conFldway == "True", 
                                sum(Shape_Area, na.rm = T) / AREA[1], 0),
         pct_conWetland = ifelse(conWetland == "True", 
                                 sum(Shape_Area, na.rm = T) / AREA[1], 0),
         pct_conPovrly = ifelse(conPovrly == "True", 
                             sum(Shape_Area, na.rm = T) / AREA[1], 0),
         pct_conCovrly = ifelse(conCovrly == "True", 
                             sum(Shape_Area, na.rm = T) / AREA[1], 0))

partial <- partial %>%
  st_drop_geometry() %>%
  group_by(STATE_ID) %>%
  summarise(pct_conFld100 = pct_conFld100[1],
            pct_conPovrly = pct_conPovrly[1],
            pct_conCovrly = pct_conCovrly[1],
            pct_conWetland = pct_conWetland[1],
            pct_conFldway = pct_conFldway[1]) %>%
  mutate(pct_conFld100 = ifelse(pct_conFld100 > 1, 1, pct_conFld100),
         pct_conFldway = ifelse(pct_conFldway > 1, 1, pct_conFldway),
         pct_conCovrly = ifelse(pct_conCovrly > 1, 1, pct_conCovrly),
         pct_conPovrly = ifelse(pct_conPovrly > 1, 1, pct_conPovrly),
         pct_conWetland = ifelse(pct_conWetland > 1, 1, pct_conWetland))

# Replace NA values with 0
partial[is.na(partial)] <- 0

# Join
fugly <- left_join(fugly, partial, by = "STATE_ID")
```


### Zoning over time

```{r}
# Load zoning shape files
zoningaug2014 <- st_read("./DATA/Zoning_History/Zoning_2014_Aug_pdx.shp")
zoningfeb2014 <- st_read("./DATA/Zoning_History/Zoning_2014_Feb_pdx.shp")
zoningaug2015 <- st_read("./DATA/Zoning_History/Zoning_2015_Aug_pdx.shp")
zoningfeb2015 <- st_read("./DATA/Zoning_History/Zoning_2015_Feb_pdx.shp")
zoningaug2016 <- st_read("./DATA/Zoning_History/Zoning_2016_Aug_pdx.shp")
zoningfeb2016 <- st_read("./DATA/Zoning_History/Zoning_2015_Feb_pdx.shp")
zoningaug2017 <- st_read("./DATA/Zoning_History/Zoning_2017_Aug_metro.shp")
zoningaug2018 <- st_read("./DATA/Zoning_History/Zoning_2018_Aug_pdx.shp")
zoningfeb2018 <- st_read("./DATA/Zoning_History/Zoning_2018_Feb_pdx.shp")

colnames(zoningaug2014) <- paste0(colnames(zoningaug2014), "_aug2014")
st_geometry(zoningaug2014) <- "geometry_aug2014"

colnames(zoningfeb2014) <- paste0(colnames(zoningfeb2014), "_feb2014")
st_geometry(zoningfeb2014) <- "geometry_feb2014"

colnames(zoningaug2015) <- paste0(colnames(zoningaug2015), "_aug2015")
st_geometry(zoningaug2015) <- "geometry_aug2015"

colnames(zoningfeb2015) <- paste0(colnames(zoningfeb2015), "_feb2015")
st_geometry(zoningfeb2015) <- "geometry_feb2015"

colnames(zoningaug2016) <- paste0(colnames(zoningaug2016), "_aug2016")
st_geometry(zoningaug2016) <- "geometry_aug2016"

colnames(zoningfeb2016) <- paste0(colnames(zoningfeb2016), "_feb2016")
st_geometry(zoningfeb2016) <- "geometry_feb2016"

colnames(zoningaug2017) <- paste0(colnames(zoningaug2017), "_aug2017")
st_geometry(zoningaug2017) <- "geometry_aug2017"

colnames(zoningaug2018) <- paste0(colnames(zoningaug2018), "_aug2018")
st_geometry(zoningaug2018) <- "geometry_aug2018"

colnames(zoningfeb2018) <- paste0(colnames(zoningfeb2018), "_feb2018")
st_geometry(zoningfeb2018) <- "geometry_feb2018"

# using st_join instead: much faster! Note this should not use the fugly df
df <- st_join(st_centroid(taxlots_pruned), zoningaug2016)
df <- st_join(df, zoningfeb2014)
df <- st_join(df, zoningaug2014)
df <- st_join(df, zoningfeb2015)
df <- st_join(df, zoningaug2015)
df <- st_join(df, zoningfeb2016)
df <- st_join(df, zoningfeb2018)
df <- st_join(df, zoningaug2017)
df <- st_join(df, zoningaug2018)

# Create new df with just the shorthand zoning variables across time
zones_over_time <- df %>%
  select(contains("ZONE_"), STATE_ID, saledate)%>%
  select(-contains("TMP")) %>%
  #select(-contains("DESC")) %>%
  select(-contains("CLASS"))

zones_over_time %<>%
  dplyr::mutate(sale_zone = case_when(saledate >= as.Date("2014-02-01") & saledate < as.Date("2014-08-01") ~ ZONE_feb2014,
                                      saledate >= as.Date("2014-08-01") & saledate < as.Date("2015-02-01") ~ ZONE_aug2014,
                                      saledate >= as.Date("2015-02-01") & saledate < as.Date("2015-08-01") ~ ZONE_feb2015,
                                      saledate >= as.Date("2015-08-01") & saledate < as.Date("2016-02-01") ~ ZONE_aug2015,
                                      saledate >= as.Date("2016-02-01") & saledate < as.Date("2016-08-01") ~ ZONE_feb2016,
                                      saledate >= as.Date("2016-08-01") & saledate < as.Date("2017-08-01") ~ ZONE_aug2016,
                                      saledate >= as.Date("2017-08-01") & saledate < as.Date("2018-02-01") ~ ZONE_aug2017,
                                      saledate >= as.Date("2018-02-01") & saledate < as.Date("2018-08-01") ~ ZONE_feb2018,
                                      saledate >= as.Date("2018-08-01") ~ ZONE_aug2018)) %>%
  dplyr::mutate(sale_zone = as.character(sale_zone)) %>%
  dplyr::mutate(sale_zone = ifelse(is.na(sale_zone), 
                                   as.character(ZONE_aug2018), 
                                   sale_zone))

# Load crosswalk from Nick sent via email 02/04
crosswalk <- read_xlsx("./DATA/zoning_crosswalk.xlsx")
crosswalk %<>%
  rename(sale_zone = `Base Zone`)

# Join use crosswalk to generate  prop_type variable: MFR, SFR, mixed use
zones_over_time %<>%
  dplyr::left_join(crosswalk, by = "sale_zone") %>%
  st_drop_geometry() %>%
  dplyr::rename(prop_type = Simplified) %>%
  # below are older zones not matched with the crosswalk but clarified
  # in email from Nick, Al, and Barry
  dplyr::mutate(prop_type = case_when(sale_zone == "CM"| sale_zone == "CG"|sale_zone == "CN1"|
                            sale_zone == "CN2"|sale_zone == "CS"|sale_zone == "CO1"|
                              sale_zone == "CO2" ~ "Mixed Use",
                          TRUE ~ prop_type)
  ) %>%
  select(-saledate)

fugly <- left_join(fugly, zones_over_time, by = "STATE_ID")
print(paste0("zone Join: ", as.character(sum(data.frame(table(fugly$STATE_ID))$Freq > 1))))

```

